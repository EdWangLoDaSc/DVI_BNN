{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/EdWangLoDaSc/DVI_BNN/blob/main/3.ipynb",
      "authorship_tag": "ABX9TyMzP3s9FM0JD8GAd0GAWkTz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/DVI_BNN/blob/main/4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex-EcKipVXiN",
        "outputId": "9cb96072-e0f5-4958-9644-2c96b8625448"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/DropoutUncertaintyExps-master\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbL7PE8HY3fh",
        "outputId": "fe074ca7-f4cf-41d5-a155-1dbfa97066ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['LICENSE',\n",
              " 'readme.md',\n",
              " 'experiment.py',\n",
              " 'Hy_Modeling_Experiments.ipynb',\n",
              " 'UCI_Datasets',\n",
              " '.vs',\n",
              " '.ipynb_checkpoints',\n",
              " 'EndUserData2021P2CWPState.csv',\n",
              " 'EndUserData2020P2CWPState.csv',\n",
              " 'EndUserData2020P2.csv',\n",
              " 'EndUserData2021P2_fix.csv',\n",
              " '__pycache__',\n",
              " 'net.py.gdoc',\n",
              " 'Hydric_Modeling',\n",
              " 'DataProcessing.py',\n",
              " 'net.py']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miceforest\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCnSoG-lpIE",
        "outputId": "004f00de-2b7c-480a-d9e7-7f5748a565f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting miceforest\n",
            "  Downloading miceforest-5.5.4-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from miceforest) (1.21.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from miceforest) (0.3.5.1)\n",
            "Collecting blosc\n",
            "  Downloading blosc-1.10.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting lightgbm>=3.3.1\n",
            "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (0.37.1)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (3.1.0)\n",
            "Installing collected packages: lightgbm, blosc, miceforest\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 2.2.3\n",
            "    Uninstalling lightgbm-2.2.3:\n",
            "      Successfully uninstalled lightgbm-2.2.3\n",
            "Successfully installed blosc-1.10.6 lightgbm-3.3.2 miceforest-5.5.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "from DataProcessing import DataProcessing as dp\n",
        "\n",
        "from subprocess import call\n",
        "import net"
      ],
      "metadata": {
        "id": "tL00fbapZcrC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_2020 = pd.read_csv('EndUserData2020P2.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "doc_2021 = pd.read_csv('EndUserData2021P2_fix.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "#print(doc_2020)\n",
        "\n",
        "doc_2020_value = dp(doc_2020, doc_2021).data_process_20().astype('float64')\n",
        "dco_2021_value = dp(doc_2020, doc_2021).data_process_21().astype('float64')\n",
        "\n",
        "BNN_20 = pd.DataFrame(np.c_[doc_2020_value['NB2_S_1_NYZ_sys_x_PcwOut_x'], doc_2020_value['NB2_S_1_NYZ_sys_x_PcwIn_x'], doc_2020_value['NB2_S_1_NYZ_cwp_9_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_10_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_11_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_12_HzSPR_x'], doc_2020_value['p_diff'], doc_2020_value['NB2_S_x_NYZ_x_x_Fcw_x']], columns = ['NB2_S_1_NYZ_sys_x_PcwOut_x', 'NB2_S_1_NYZ_sys_x_PcwIn_x', 'NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x', 'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x', 'diff', 'NB2_S_x_NYZ_x_x_Fcw_x'])\n",
        "BNN_21 = pd.DataFrame(np.c_[dco_2021_value['NB2_S_1_NYZ_sys_x_PcwOut_x'], dco_2021_value['NB2_S_1_NYZ_sys_x_PcwIn_x'], dco_2021_value['NB2_S_1_NYZ_cwp_9_HzSPR_x'], dco_2021_value['NB2_S_1_NYZ_cwp_10_HzSPR_x'], dco_2021_value['NB2_S_1_NYZ_cwp_11_HzSPR_x'],  dco_2021_value['NB2_S_1_NYZ_cwp_12_HzSPR_x'],  dco_2021_value['p_diff'], dco_2021_value['NB2_S_x_NYZ_x_x_Fcw_x']], columns = ['NB2_S_1_NYZ_sys_x_PcwOut_x', 'NB2_S_1_NYZ_sys_x_PcwIn_x', 'NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x', 'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x', 'diff', 'NB2_S_x_NYZ_x_x_Fcw_x'])\n"
      ],
      "metadata": {
        "id": "uebW7cppY80Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp.dataframe_to_txt(BNN_20)\n"
      ],
      "metadata": {
        "id": "BcOfJU7wo-A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dp.split_data_train_test('data_file.txt')\n"
      ],
      "metadata": {
        "id": "N3qauXcy3KN3",
        "outputId": "1c106ac1-3943-404d-bbb8-2397b5694e93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/First Version/Data\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "z0WYDm6TP3m-",
        "outputId": "af702d2d-af7e-441d-f0ac-de1d0cf3fed3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data_file.txt',\n",
              " 'index_train_0.txt',\n",
              " 'index_test_0.txt',\n",
              " 'index_train_1.txt',\n",
              " 'index_test_1.txt',\n",
              " 'index_train_2.txt',\n",
              " 'index_test_2.txt',\n",
              " 'index_train_3.txt',\n",
              " 'index_test_3.txt',\n",
              " 'index_train_4.txt',\n",
              " 'index_test_4.txt',\n",
              " 'index_train_5.txt',\n",
              " 'index_test_5.txt',\n",
              " 'index_train_6.txt',\n",
              " 'index_test_6.txt',\n",
              " 'index_train_7.txt',\n",
              " 'index_test_7.txt',\n",
              " 'index_train_8.txt',\n",
              " 'index_test_8.txt',\n",
              " 'index_train_9.txt',\n",
              " 'index_test_9.txt',\n",
              " 'index_train_10.txt',\n",
              " 'index_test_10.txt',\n",
              " 'index_train_11.txt',\n",
              " 'index_test_11.txt',\n",
              " 'index_train_12.txt',\n",
              " 'index_test_12.txt',\n",
              " 'index_train_13.txt',\n",
              " 'index_test_13.txt',\n",
              " 'index_train_14.txt',\n",
              " 'index_test_14.txt',\n",
              " 'index_train_15.txt',\n",
              " 'index_test_15.txt',\n",
              " 'index_train_16.txt',\n",
              " 'index_test_16.txt',\n",
              " 'index_train_17.txt',\n",
              " 'index_test_17.txt',\n",
              " 'index_train_18.txt',\n",
              " 'index_test_18.txt',\n",
              " 'index_train_19.txt',\n",
              " 'index_test_19.txt',\n",
              " 'n_splits.txt',\n",
              " 'index_features.txt',\n",
              " 'index_target.txt',\n",
              " 'dropout_rates.txt',\n",
              " 'tau_values.txt',\n",
              " 'n_hidden.txt',\n",
              " 'n_epochs.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "smWU291RVS9s",
        "outputId": "8f98b67b-c2a4-41eb-8a39-c0dfbae0801a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing existing result files...\n",
            "Result files removed.\n",
            "Loading data and other hyperparameters...\n",
            "Done.\n",
            "Loading file: index_train_0.txt\n",
            "Loading file: index_test_0.txt\n",
            "Number of training examples: 56294\n",
            "Number of validation examples: 14074\n",
            "Number of test examples: 7819\n",
            "Number of train_original examples: 70368\n",
            "Grid search step: Tau: 0.025 Dropout rate: 0.005\n",
            "29/29 [==============================] - 0s 2ms/step\n",
            "Best log_likelihood changed to: -14.681445330108168\n",
            "Best tau changed to: 0.025\n",
            "Best dropout rate changed to: 0.005\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-60d87b0099c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# Storing validation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_RESULTS_VALIDATION_RMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dropout_Rate: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Tau: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' :: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/First Version/results/validation_rmse_10_xepochs_3_hidden_layers.txt'"
          ]
        }
      ],
      "source": [
        "# Copyright 2016, Yarin Gal, All rights reserved.\n",
        "# This code is based on the code by Jose Miguel Hernandez-Lobato used for his \n",
        "# paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\".\n",
        "\n",
        "# This file contains code to train dropout networks on the UCI datasets using the following algorithm:\n",
        "# 1. Create 20 random splits of the training-test dataset.\n",
        "# 2. For each split:\n",
        "# 3.   Create a validation (val) set taking 20% of the training set.\n",
        "# 4.   Get best hyperparameters: dropout_rate and tau by training on (train-val) set and testing on val set.\n",
        "# 5.   Train a network on the entire training set with the best pair of hyperparameters.\n",
        "# 6.   Get the performance (MC RMSE and log-likelihood) on the test set.\n",
        "# 7. Report the averaged performance (Monte Carlo RMSE and log-likelihood) on all 20 splits.\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "import pandas as pd\n",
        "from DataProcessing import DataProcessing as dp\n",
        "\n",
        "from subprocess import call\n",
        "import net\n",
        "\n",
        "parser=argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--dir', '-d', required=True, help='Name of the UCI Dataset directory. Eg')\n",
        "parser.add_argument('--epochx','-e', default=500, type=int, help='Multiplier for the number of epochs for training.')\n",
        "parser.add_argument('--hidden', '-nh', default=2, type=int, help='Number of hidden layers for the neural net')\n",
        "\n",
        "args = parser.parse_args(args=['--dir','First Version','--epochx','10','--hidden','3'])\n",
        "\n",
        "data_directory = args.dir\n",
        "epochs_multiplier = args.epochx\n",
        "num_hidden_layers = args.hidden\n",
        "\n",
        "sys.path.append('net/')\n",
        "\n",
        "_RESULTS_VALIDATION_LL = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/validation_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_VALIDATION_RMSE = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/validation_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_VALIDATION_MC_RMSE = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/validation_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "\n",
        "#_RESULTS_TEST_LL = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/\n",
        "_RESULTS_TEST_LL = \"test_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "#_RESULTS_TEST_TAU = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/test_tau_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_TAU = \"test_tau_\" + str(epochs_multiplier) + \"_xepo_chs\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "\n",
        "_RESULTS_TEST_RMSE = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/test_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_MC_RMSE = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/test_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_LOG = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/results/log_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "\n",
        "_DATA_DIRECTORY_PATH = \"./content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/\" + data_directory + \"/Data/\"\n",
        "_DROPOUT_RATES_FILE = _DATA_DIRECTORY_PATH + \"dropout_rates.txt\"\n",
        "_TAU_VALUES_FILE = _DATA_DIRECTORY_PATH + \"tau_values.txt\"\n",
        "_DATA_FILE = _DATA_DIRECTORY_PATH + \"data_file.txt\"\n",
        "_HIDDEN_UNITS_FILE = _DATA_DIRECTORY_PATH + \"n_hidden.txt\"\n",
        "_EPOCHS_FILE = _DATA_DIRECTORY_PATH + \"n_epochs.txt\"\n",
        "_INDEX_FEATURES_FILE = _DATA_DIRECTORY_PATH + \"index_features.txt\"\n",
        "_INDEX_TARGET_FILE = _DATA_DIRECTORY_PATH + \"index_target.txt\"\n",
        "_N_SPLITS_FILE = _DATA_DIRECTORY_PATH + \"n_splits.txt\"\n",
        "\n",
        "def _get_index_train_test_path(split_num, train = True):\n",
        "    \"\"\"\n",
        "       Method to generate the path containing the training/test split for the given\n",
        "       split number (generally from 1 to 20).\n",
        "       @param split_num      Split number for which the data has to be generated\n",
        "       @param train          Is true if the data is training data. Else false.\n",
        "       @return path          Path of the file containing the requried data\n",
        "    \"\"\"\n",
        "    if train:\n",
        "        return \"index_train_\" + str(split_num) + \".txt\"\n",
        "    else:\n",
        "        return \"index_test_\" + str(split_num) + \".txt\" \n",
        "\n",
        "\n",
        "print (\"Removing existing result files...\")\n",
        "call([\"rm\", _RESULTS_VALIDATION_LL])\n",
        "call([\"rm\", _RESULTS_VALIDATION_RMSE])\n",
        "call([\"rm\", _RESULTS_VALIDATION_MC_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_LL])\n",
        "call([\"rm\", _RESULTS_TEST_TAU])\n",
        "call([\"rm\", _RESULTS_TEST_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_MC_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_LOG])\n",
        "print (\"Result files removed.\")\n",
        "\n",
        "# We fix the random seed\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "print (\"Loading data and other hyperparameters...\")\n",
        "# We load the data\n",
        "\n",
        "data = np.loadtxt('data_file.txt')\n",
        "\n",
        "# We load the number of hidden units\n",
        "\n",
        "n_hidden = np.loadtxt(\"n_hidden.txt\").tolist()\n",
        "\n",
        "# We load the number of training epocs\n",
        "\n",
        "n_epochs = np.loadtxt(\"n_epochs.txt\").tolist()\n",
        "\n",
        "# We load the indexes for the features and for the target\n",
        "\n",
        "index_features = np.loadtxt('index_features.txt')\n",
        "index_target = np.loadtxt(\"index_target.txt\")\n",
        "\n",
        "X = data[ : , [int(i) for i in index_features.tolist()] ]\n",
        "y = data[ : , int(index_target.tolist()) ]\n",
        "\n",
        "# We iterate over the training test splits\n",
        "\n",
        "n_splits = np.loadtxt(\"n_splits.txt\")\n",
        "print (\"Done.\")\n",
        "\n",
        "errors, MC_errors, lls = [], [], []\n",
        "for split in range(int(n_splits)):\n",
        "\n",
        "    # We load the indexes of the training and test sets\n",
        "    print ('Loading file: ' + _get_index_train_test_path(split, train=True))\n",
        "    print ('Loading file: ' + _get_index_train_test_path(split, train=False))\n",
        "    index_train = np.loadtxt(_get_index_train_test_path(split, train=True))\n",
        "    index_test = np.loadtxt(_get_index_train_test_path(split, train=False))\n",
        "\n",
        "    X_train = X[ [int(i) for i in index_train.tolist()] ]\n",
        "    y_train = y[ [int(i) for i in index_train.tolist()] ]\n",
        "    \n",
        "    X_test = X[ [int(i) for i in index_test.tolist()] ]\n",
        "    y_test = y[ [int(i) for i in index_test.tolist()] ]\n",
        "\n",
        "    X_train_original = X_train\n",
        "    y_train_original = y_train\n",
        "    num_training_examples = int(0.8 * X_train.shape[0])\n",
        "    X_validation = X_train[num_training_examples:, :]\n",
        "    y_validation = y_train[num_training_examples:]\n",
        "    X_train = X_train[0:num_training_examples, :]\n",
        "    y_train = y_train[0:num_training_examples]\n",
        "    \n",
        "    # Printing the size of the training, validation and test sets\n",
        "    print ('Number of training examples: ' + str(X_train.shape[0]))\n",
        "    print ('Number of validation examples: ' + str(X_validation.shape[0]))\n",
        "    print ('Number of test examples: ' + str(X_test.shape[0]))\n",
        "    print ('Number of train_original examples: ' + str(X_train_original.shape[0]))\n",
        "\n",
        "    # List of hyperparameters which we will try out using grid-search\n",
        "    dropout_rates = np.loadtxt(\"dropout_rates.txt\").tolist()\n",
        "    tau_values = np.loadtxt(\"tau_values.txt\").tolist()\n",
        "\n",
        "    # We perform grid-search to select the best hyperparameters based on the highest log-likelihood value\n",
        "    best_network = None\n",
        "    best_ll = -float('inf')\n",
        "    best_tau = 0\n",
        "    best_dropout = 0\n",
        "    for dropout_rate in dropout_rates:\n",
        "        for tau in tau_values:\n",
        "            print ('Grid search step: Tau: ' + str(tau) + ' Dropout rate: ' + str(dropout_rate))\n",
        "            network = net.net(X_train, y_train, ([ int(n_hidden) ] * num_hidden_layers),\n",
        "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = tau,\n",
        "                    dropout = dropout_rate)\n",
        "\n",
        "            # We obtain the test RMSE and the test ll from the validation sets\n",
        "\n",
        "            error, MC_error, ll = network.predict(X_validation, y_validation)\n",
        "            if (ll > best_ll):\n",
        "                best_ll = ll\n",
        "                best_network = network\n",
        "                best_tau = tau\n",
        "                best_dropout = dropout_rate\n",
        "                print ('Best log_likelihood changed to: ' + str(best_ll))\n",
        "                print ('Best tau changed to: ' + str(best_tau))\n",
        "                print ('Best dropout rate changed to: ' + str(best_dropout))\n",
        "            \n",
        "            # Storing validation results\n",
        "            with open(_RESULTS_VALIDATION_RMSE, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(error) + '\\n')\n",
        "\n",
        "            with open(_RESULTS_VALIDATION_MC_RMSE, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(MC_error) + '\\n')\n",
        "\n",
        "            with open(_RESULTS_VALIDATION_LL, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(ll) + '\\n')\n",
        "\n",
        "    # Storing test results\n",
        "    best_network = net.net(X_train_original, y_train_original, ([ int(n_hidden) ] * num_hidden_layers),\n",
        "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = best_tau,\n",
        "                    dropout = best_dropout)\n",
        "    error, MC_error, ll = best_network.predict(X_test, y_test)\n",
        "    \n",
        "    with open(_RESULTS_TEST_RMSE, \"a\") as myfile:\n",
        "        myfile.write(repr(error) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_MC_RMSE, \"a\") as myfile:\n",
        "        myfile.write(repr(MC_error) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_LL, \"a\") as myfile:\n",
        "        myfile.write(repr(ll) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_TAU, \"a\") as myfile:\n",
        "        myfile.write(repr(best_network.tau) + '\\n')\n",
        "\n",
        "    print (\"Tests on split \" + str(split) + \" complete.\")\n",
        "    errors += [error]\n",
        "    MC_errors += [MC_error]\n",
        "    lls += [ll]\n",
        "\n",
        "with open(_RESULTS_TEST_LOG, \"a\") as myfile:\n",
        "    myfile.write('errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(errors), np.std(errors), np.std(errors)/math.sqrt(n_splits),\n",
        "        np.percentile(errors, 50), np.percentile(errors, 25), np.percentile(errors, 75)))\n",
        "    myfile.write('MC errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(MC_errors), np.std(MC_errors), np.std(MC_errors)/math.sqrt(n_splits),\n",
        "        np.percentile(MC_errors, 50), np.percentile(MC_errors, 25), np.percentile(MC_errors, 75)))\n",
        "    myfile.write('lls %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(lls), np.std(lls), np.std(lls)/math.sqrt(n_splits), \n",
        "        np.percentile(lls, 50), np.percentile(lls, 25), np.percentile(lls, 75)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/DropoutUncertaintyExps-master/Hydric_Modeling/First Version/Data\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "id": "fucErt8oWAPw",
        "outputId": "0a562698-47a5-434d-d62a-805701136ff5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data_file.txt',\n",
              " 'index_train_0.txt',\n",
              " 'index_test_0.txt',\n",
              " 'index_train_1.txt',\n",
              " 'index_test_1.txt',\n",
              " 'index_train_2.txt',\n",
              " 'index_test_2.txt',\n",
              " 'index_train_3.txt',\n",
              " 'index_test_3.txt',\n",
              " 'index_train_4.txt',\n",
              " 'index_test_4.txt',\n",
              " 'index_train_5.txt',\n",
              " 'index_test_5.txt',\n",
              " 'index_train_6.txt',\n",
              " 'index_test_6.txt',\n",
              " 'index_train_7.txt',\n",
              " 'index_test_7.txt',\n",
              " 'index_train_8.txt',\n",
              " 'index_test_8.txt',\n",
              " 'index_train_9.txt',\n",
              " 'index_test_9.txt',\n",
              " 'index_train_10.txt',\n",
              " 'index_test_10.txt',\n",
              " 'index_train_11.txt',\n",
              " 'index_test_11.txt',\n",
              " 'index_train_12.txt',\n",
              " 'index_test_12.txt',\n",
              " 'index_train_13.txt',\n",
              " 'index_test_13.txt',\n",
              " 'index_train_14.txt',\n",
              " 'index_test_14.txt',\n",
              " 'index_train_15.txt',\n",
              " 'index_test_15.txt',\n",
              " 'index_train_16.txt',\n",
              " 'index_test_16.txt',\n",
              " 'index_train_17.txt',\n",
              " 'index_test_17.txt',\n",
              " 'index_train_18.txt',\n",
              " 'index_test_18.txt',\n",
              " 'index_train_19.txt',\n",
              " 'index_test_19.txt',\n",
              " 'n_splits.txt',\n",
              " 'index_features.txt',\n",
              " 'index_target.txt',\n",
              " 'n_hidden.txt',\n",
              " 'n_epochs.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt('data_file.txt')"
      ],
      "metadata": {
        "id": "8q1nk3WsVn8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}