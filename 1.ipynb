{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPcSWGQaRK3aZv7j+IvudQZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdWangLoDaSc/DVI_BNN/blob/main/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex-EcKipVXiN",
        "outputId": "36b3b4e1-6665-4832-a69b-c2ed5de019ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path=\"/content/drive/MyDrive/DropoutUncertaintyExps-master\"\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbL7PE8HY3fh",
        "outputId": "5ddd961d-6b81-43e1-87af-6131acf2b106"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DataProcessing.py',\n",
              " 'net.py',\n",
              " 'experiment.py',\n",
              " 'readme.md',\n",
              " 'LICENSE',\n",
              " 'Hy_Modeling_Experiments.ipynb',\n",
              " '.vs',\n",
              " 'UCI_Datasets',\n",
              " '.ipynb_checkpoints',\n",
              " 'EndUserData2020P2CWPState.csv',\n",
              " 'EndUserData2021P2_fix.csv',\n",
              " 'EndUserData2020P2.csv',\n",
              " 'EndUserData2021P2CWPState.csv',\n",
              " '__pycache__',\n",
              " 'net.py.gdoc']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install miceforest\n",
        "!pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cCnSoG-lpIE",
        "outputId": "fd4f16a1-2f91-43d5-9462-a30451250144"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: miceforest in /usr/local/lib/python3.7/dist-packages (5.5.4)\n",
            "Requirement already satisfied: lightgbm>=3.3.1 in /usr/local/lib/python3.7/dist-packages (from miceforest) (3.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from miceforest) (1.21.6)\n",
            "Requirement already satisfied: blosc in /usr/local/lib/python3.7/dist-packages (from miceforest) (1.10.6)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from miceforest) (0.3.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (1.0.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm>=3.3.1->miceforest) (0.37.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.22.0->lightgbm>=3.3.1->miceforest) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "from DataProcessing import DataProcessing as dp\n",
        "\n",
        "from subprocess import call\n",
        "import net"
      ],
      "metadata": {
        "id": "tL00fbapZcrC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_2020 = pd.read_csv('EndUserData2020P2.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "doc_2021 = pd.read_csv('EndUserData2021P2_fix.csv', encoding = 'ISO-8859-1', skiprows = [0,2]).astype('float')\n",
        "#print(doc_2020)\n",
        "\n",
        "doc_2020_value = dp(doc_2020, doc_2021).data_process_20().astype('float64')\n",
        "dco_2021_value = dp(doc_2020, doc_2021).data_process_21().astype('float64')\n",
        "\n",
        "BNN_20 = pd.DataFrame(np.c_[doc_2020_value['NB2_S_1_NYZ_sys_x_PcwOut_x'], doc_2020_value['NB2_S_1_NYZ_sys_x_PcwIn_x'], doc_2020_value['NB2_S_1_NYZ_cwp_9_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_10_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_11_HzSPR_x'], doc_2020_value['NB2_S_1_NYZ_cwp_12_HzSPR_x'], doc_2020_value['p_diff'], doc_2020_value['NB2_S_x_NYZ_x_x_Fcw_x']], columns = ['NB2_S_1_NYZ_sys_x_PcwOut_x', 'NB2_S_1_NYZ_sys_x_PcwIn_x', 'NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x', 'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x', 'diff', 'NB2_S_x_NYZ_x_x_Fcw_x'])\n",
        "BNN_21 = pd.DataFrame(np.c_[dco_2021_value['NB2_S_1_NYZ_sys_x_PcwOut_x'], dco_2021_value['NB2_S_1_NYZ_sys_x_PcwIn_x'], dco_2021_value['NB2_S_1_NYZ_cwp_9_HzSPR_x'], dco_2021_value['NB2_S_1_NYZ_cwp_10_HzSPR_x'], dco_2021_value['NB2_S_1_NYZ_cwp_11_HzSPR_x'],  dco_2021_value['NB2_S_1_NYZ_cwp_12_HzSPR_x'],  dco_2021_value['p_diff'], dco_2021_value['NB2_S_x_NYZ_x_x_Fcw_x']], columns = ['NB2_S_1_NYZ_sys_x_PcwOut_x', 'NB2_S_1_NYZ_sys_x_PcwIn_x', 'NB2_S_1_NYZ_cwp_9_HzSPR_x', 'NB2_S_1_NYZ_cwp_10_HzSPR_x', 'NB2_S_1_NYZ_cwp_11_HzSPR_x', 'NB2_S_1_NYZ_cwp_12_HzSPR_x', 'diff', 'NB2_S_x_NYZ_x_x_Fcw_x'])\n"
      ],
      "metadata": {
        "id": "uebW7cppY80Z"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(BNN_20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqGxTx3UvbV6",
        "outputId": "5b097471-b435-4ce2-e6b7-f136b246ce40"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       NB2_S_1_NYZ_sys_x_PcwOut_x  NB2_S_1_NYZ_sys_x_PcwIn_x  \\\n",
            "0                            4.43                     3.8194   \n",
            "1                            4.42                     3.8281   \n",
            "2                            4.43                     3.8194   \n",
            "3                            4.42                     3.8108   \n",
            "4                            4.43                     3.8281   \n",
            "...                           ...                        ...   \n",
            "78182                        5.16                     4.3056   \n",
            "78183                        5.17                     4.3142   \n",
            "78184                        5.16                     4.3142   \n",
            "78185                        5.16                     4.3056   \n",
            "78186                        5.16                     4.2969   \n",
            "\n",
            "       NB2_S_1_NYZ_cwp_9_HzSPR_x  NB2_S_1_NYZ_cwp_10_HzSPR_x  \\\n",
            "0                            0.0                      0.0000   \n",
            "1                            0.0                      0.0000   \n",
            "2                            0.0                      0.0000   \n",
            "3                            0.0                      0.0000   \n",
            "4                            0.0                      0.0000   \n",
            "...                          ...                         ...   \n",
            "78182                       28.9                     28.9094   \n",
            "78183                       28.9                     28.9121   \n",
            "78184                       28.9                     28.8927   \n",
            "78185                       28.9                     28.8824   \n",
            "78186                       28.9                     28.8825   \n",
            "\n",
            "       NB2_S_1_NYZ_cwp_11_HzSPR_x  NB2_S_1_NYZ_cwp_12_HzSPR_x    diff  \\\n",
            "0                         26.8020                     26.8020  0.6106   \n",
            "1                         26.7771                     26.7771  0.5919   \n",
            "2                         26.7873                     26.7873  0.6106   \n",
            "3                         26.7825                     26.7825  0.6092   \n",
            "4                         26.7585                     26.7585  0.6019   \n",
            "...                           ...                         ...     ...   \n",
            "78182                     28.9034                      0.0000  0.8544   \n",
            "78183                     28.9105                      0.0000  0.8558   \n",
            "78184                     28.8927                      0.0000  0.8458   \n",
            "78185                     28.8824                      0.0000  0.8544   \n",
            "78186                     28.8825                      0.0000  0.8631   \n",
            "\n",
            "       NB2_S_x_NYZ_x_x_Fcw_x  \n",
            "0                    2038.37  \n",
            "1                    2038.37  \n",
            "2                    2038.37  \n",
            "3                    2041.04  \n",
            "4                    2041.04  \n",
            "...                      ...  \n",
            "78182                3093.46  \n",
            "78183                3067.41  \n",
            "78184                3067.41  \n",
            "78185                3067.41  \n",
            "78186                3050.44  \n",
            "\n",
            "[78187 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dp.dataframe_to_txt(BNN_20)\n",
        "\n"
      ],
      "metadata": {
        "id": "BcOfJU7wo-A-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smWU291RVS9s"
      },
      "outputs": [],
      "source": [
        "# Copyright 2016, Yarin Gal, All rights reserved.\n",
        "# This code is based on the code by Jose Miguel Hernandez-Lobato used for his \n",
        "# paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\".\n",
        "\n",
        "# This file contains code to train dropout networks on the UCI datasets using the following algorithm:\n",
        "# 1. Create 20 random splits of the training-test dataset.\n",
        "# 2. For each split:\n",
        "# 3.   Create a validation (val) set taking 20% of the training set.\n",
        "# 4.   Get best hyperparameters: dropout_rate and tau by training on (train-val) set and testing on val set.\n",
        "# 5.   Train a network on the entire training set with the best pair of hyperparameters.\n",
        "# 6.   Get the performance (MC RMSE and log-likelihood) on the test set.\n",
        "# 7. Report the averaged performance (Monte Carlo RMSE and log-likelihood) on all 20 splits.\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "from DataProcessing import DataProcessing as dp\n",
        "\n",
        "from subprocess import call\n",
        "import net\n",
        "\n",
        "parser=argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--dir', '-d', required=True, help='Name of the UCI Dataset directory. Eg')\n",
        "parser.add_argument('--epochx','-e', default=500, type=int, help='Multiplier for the number of epochs for training.')\n",
        "parser.add_argument('--hidden', '-nh', default=2, type=int, help='Number of hidden layers for the neural net')\n",
        "\n",
        "args=parser.parse_args()\n",
        "\n",
        "data_directory = args.dir\n",
        "epochs_multiplier = args.epochx\n",
        "num_hidden_layers = args.hidden\n",
        "\n",
        "sys.path.append('net/')\n",
        "\n",
        "_RESULTS_VALIDATION_LL = \"./Hydric_Datasets/\" + data_directory + \"/results/validation_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_VALIDATION_RMSE = \"./Hydric_Datasets/\" + data_directory + \"/results/validation_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_VALIDATION_MC_RMSE = \"./Hydric_Datasets/\" + data_directory + \"/results/validation_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "\n",
        "_RESULTS_TEST_LL = \"./Hydric_Datasets/\" + data_directory + \"/results/test_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_TAU = \"./Hydric_Datasets/\" + data_directory + \"/results/test_tau_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_RMSE = \"./Hydric_Datasets/\" + data_directory + \"/results/test_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_MC_RMSE = \"./Hydric_Datasets/\" + data_directory + \"/results/test_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "_RESULTS_TEST_LOG = \"./Hydric_Datasets/\" + data_directory + \"/results/log_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
        "\n",
        "_DATA_DIRECTORY_PATH = \"./Hydric_Datasets/\" + data_directory + \"/data/\"\n",
        "_DROPOUT_RATES_FILE = _DATA_DIRECTORY_PATH + \"dropout_rates.txt\"\n",
        "_TAU_VALUES_FILE = _DATA_DIRECTORY_PATH + \"tau_values.txt\"\n",
        "_DATA_FILE = _DATA_DIRECTORY_PATH + \"data.txt\"\n",
        "_HIDDEN_UNITS_FILE = _DATA_DIRECTORY_PATH + \"n_hidden.txt\"\n",
        "_EPOCHS_FILE = _DATA_DIRECTORY_PATH + \"n_epochs.txt\"\n",
        "_INDEX_FEATURES_FILE = _DATA_DIRECTORY_PATH + \"index_features.txt\"\n",
        "_INDEX_TARGET_FILE = _DATA_DIRECTORY_PATH + \"index_target.txt\"\n",
        "_N_SPLITS_FILE = _DATA_DIRECTORY_PATH + \"n_splits.txt\"\n",
        "\n",
        "def _get_index_train_test_path(split_num, train = True):\n",
        "    \"\"\"\n",
        "       Method to generate the path containing the training/test split for the given\n",
        "       split number (generally from 1 to 20).\n",
        "       @param split_num      Split number for which the data has to be generated\n",
        "       @param train          Is true if the data is training data. Else false.\n",
        "       @return path          Path of the file containing the requried data\n",
        "    \"\"\"\n",
        "    if train:\n",
        "        return _DATA_DIRECTORY_PATH + \"index_train_\" + str(split_num) + \".txt\"\n",
        "    else:\n",
        "        return _DATA_DIRECTORY_PATH + \"index_test_\" + str(split_num) + \".txt\" \n",
        "\n",
        "\n",
        "print (\"Removing existing result files...\")\n",
        "call([\"rm\", _RESULTS_VALIDATION_LL])\n",
        "call([\"rm\", _RESULTS_VALIDATION_RMSE])\n",
        "call([\"rm\", _RESULTS_VALIDATION_MC_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_LL])\n",
        "call([\"rm\", _RESULTS_TEST_TAU])\n",
        "call([\"rm\", _RESULTS_TEST_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_MC_RMSE])\n",
        "call([\"rm\", _RESULTS_TEST_LOG])\n",
        "print (\"Result files removed.\")\n",
        "\n",
        "# We fix the random seed\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "print (\"Loading data and other hyperparameters...\")\n",
        "# We load the data\n",
        "\n",
        "data = np.loadtxt(_DATA_FILE)\n",
        "\n",
        "# We load the number of hidden units\n",
        "\n",
        "n_hidden = np.loadtxt(_HIDDEN_UNITS_FILE).tolist()\n",
        "\n",
        "# We load the number of training epocs\n",
        "\n",
        "n_epochs = np.loadtxt(_EPOCHS_FILE).tolist()\n",
        "\n",
        "# We load the indexes for the features and for the target\n",
        "\n",
        "index_features = np.loadtxt(_INDEX_FEATURES_FILE)\n",
        "index_target = np.loadtxt(_INDEX_TARGET_FILE)\n",
        "\n",
        "X = data[ : , [int(i) for i in index_features.tolist()] ]\n",
        "y = data[ : , int(index_target.tolist()) ]\n",
        "\n",
        "# We iterate over the training test splits\n",
        "\n",
        "n_splits = np.loadtxt(_N_SPLITS_FILE)\n",
        "print (\"Done.\")\n",
        "\n",
        "errors, MC_errors, lls = [], [], []\n",
        "for split in range(int(n_splits)):\n",
        "\n",
        "    # We load the indexes of the training and test sets\n",
        "    print ('Loading file: ' + _get_index_train_test_path(split, train=True))\n",
        "    print ('Loading file: ' + _get_index_train_test_path(split, train=False))\n",
        "    index_train = np.loadtxt(_get_index_train_test_path(split, train=True))\n",
        "    index_test = np.loadtxt(_get_index_train_test_path(split, train=False))\n",
        "\n",
        "    X_train = X[ [int(i) for i in index_train.tolist()] ]\n",
        "    y_train = y[ [int(i) for i in index_train.tolist()] ]\n",
        "    \n",
        "    X_test = X[ [int(i) for i in index_test.tolist()] ]\n",
        "    y_test = y[ [int(i) for i in index_test.tolist()] ]\n",
        "\n",
        "    X_train_original = X_train\n",
        "    y_train_original = y_train\n",
        "    num_training_examples = int(0.8 * X_train.shape[0])\n",
        "    X_validation = X_train[num_training_examples:, :]\n",
        "    y_validation = y_train[num_training_examples:]\n",
        "    X_train = X_train[0:num_training_examples, :]\n",
        "    y_train = y_train[0:num_training_examples]\n",
        "    \n",
        "    # Printing the size of the training, validation and test sets\n",
        "    print ('Number of training examples: ' + str(X_train.shape[0]))\n",
        "    print ('Number of validation examples: ' + str(X_validation.shape[0]))\n",
        "    print ('Number of test examples: ' + str(X_test.shape[0]))\n",
        "    print ('Number of train_original examples: ' + str(X_train_original.shape[0]))\n",
        "\n",
        "    # List of hyperparameters which we will try out using grid-search\n",
        "    dropout_rates = np.loadtxt(_DROPOUT_RATES_FILE).tolist()\n",
        "    tau_values = np.loadtxt(_TAU_VALUES_FILE).tolist()\n",
        "\n",
        "    # We perform grid-search to select the best hyperparameters based on the highest log-likelihood value\n",
        "    best_network = None\n",
        "    best_ll = -float('inf')\n",
        "    best_tau = 0\n",
        "    best_dropout = 0\n",
        "    for dropout_rate in dropout_rates:\n",
        "        for tau in tau_values:\n",
        "            print ('Grid search step: Tau: ' + str(tau) + ' Dropout rate: ' + str(dropout_rate))\n",
        "            network = net.net(X_train, y_train, ([ int(n_hidden) ] * num_hidden_layers),\n",
        "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = tau,\n",
        "                    dropout = dropout_rate)\n",
        "\n",
        "            # We obtain the test RMSE and the test ll from the validation sets\n",
        "\n",
        "            error, MC_error, ll = network.predict(X_validation, y_validation)\n",
        "            if (ll > best_ll):\n",
        "                best_ll = ll\n",
        "                best_network = network\n",
        "                best_tau = tau\n",
        "                best_dropout = dropout_rate\n",
        "                print ('Best log_likelihood changed to: ' + str(best_ll))\n",
        "                print ('Best tau changed to: ' + str(best_tau))\n",
        "                print ('Best dropout rate changed to: ' + str(best_dropout))\n",
        "            \n",
        "            # Storing validation results\n",
        "            with open(_RESULTS_VALIDATION_RMSE, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(error) + '\\n')\n",
        "\n",
        "            with open(_RESULTS_VALIDATION_MC_RMSE, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(MC_error) + '\\n')\n",
        "\n",
        "            with open(_RESULTS_VALIDATION_LL, \"a\") as myfile:\n",
        "                myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
        "                myfile.write(repr(ll) + '\\n')\n",
        "\n",
        "    # Storing test results\n",
        "    best_network = net.net(X_train_original, y_train_original, ([ int(n_hidden) ] * num_hidden_layers),\n",
        "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = best_tau,\n",
        "                    dropout = best_dropout)\n",
        "    error, MC_error, ll = best_network.predict(X_test, y_test)\n",
        "    \n",
        "    with open(_RESULTS_TEST_RMSE, \"a\") as myfile:\n",
        "        myfile.write(repr(error) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_MC_RMSE, \"a\") as myfile:\n",
        "        myfile.write(repr(MC_error) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_LL, \"a\") as myfile:\n",
        "        myfile.write(repr(ll) + '\\n')\n",
        "\n",
        "    with open(_RESULTS_TEST_TAU, \"a\") as myfile:\n",
        "        myfile.write(repr(best_network.tau) + '\\n')\n",
        "\n",
        "    print (\"Tests on split \" + str(split) + \" complete.\")\n",
        "    errors += [error]\n",
        "    MC_errors += [MC_error]\n",
        "    lls += [ll]\n",
        "\n",
        "with open(_RESULTS_TEST_LOG, \"a\") as myfile:\n",
        "    myfile.write('errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(errors), np.std(errors), np.std(errors)/math.sqrt(n_splits),\n",
        "        np.percentile(errors, 50), np.percentile(errors, 25), np.percentile(errors, 75)))\n",
        "    myfile.write('MC errors %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(MC_errors), np.std(MC_errors), np.std(MC_errors)/math.sqrt(n_splits),\n",
        "        np.percentile(MC_errors, 50), np.percentile(MC_errors, 25), np.percentile(MC_errors, 75)))\n",
        "    myfile.write('lls %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
        "        np.mean(lls), np.std(lls), np.std(lls)/math.sqrt(n_splits), \n",
        "        np.percentile(lls, 50), np.percentile(lls, 25), np.percentile(lls, 75)))\n",
        "\n"
      ]
    }
  ]
}